TODO, LLM chatbot
    ChatGPT was able to provide a good roadmap for working from simple to more complex; With the caveat of at each step of abstraction, I want to make use of popular and powerful tools to complete a project or challenge(s) that would contribute to my understanding, as well as make a "minimal viable project" (MVP) at each step.

    Where the roadmap is focused on developing expertise in neural networks (NN), then natural language processing (NLP), deep learning, transformer models, creating a progressively more complex chat bot, making a polished interface, and finishing with deployment and scaling.

    Roadmap
        Step-by-Step Roadmap (Revised)
            Basic Neural Network Implementation

            Objective: Understand the fundamentals of neural networks.
            Project: Implement a simple feedforward neural network from scratch.
            Skills Learned: Backpropagation, gradient descent, activation functions.
            Technologies: Pure Python, NumPy.
            Using a Deep Learning Framework

            Objective: Get familiar with a popular deep learning framework.
            Project: Re-implement the basic neural network using PyTorch or TensorFlow.
            Skills Learned: Using tensors, automatic differentiation, basic model training.
            Technologies: PyTorch, TensorFlow.
            Text Data Preprocessing

            Objective: Learn how to preprocess text data for NLP tasks.
            Project: Implement text preprocessing techniques such as tokenization, stemming, lemmatization, and stop words removal.
            Skills Learned: Text normalization, handling different text formats.
            Technologies: NLTK, SpaCy.
            Word Embeddings and Text Representation

            Objective: Understand how to represent text data in a way that neural networks can process.
            Project: Implement and use word embeddings like Word2Vec or GloVe for text representation.
            Skills Learned: Embedding layers, vector space models.
            Technologies: Gensim, PyTorch, TensorFlow.
            Recurrent Neural Networks (RNNs) and LSTMs

            Objective: Learn about RNNs for handling sequential data.
            Project: Build an RNN or LSTM for a simple text generation task.
            Skills Learned: Handling sequences, managing hidden states, text generation.
            Technologies: PyTorch, TensorFlow.
            Transformer Models

            Objective: Dive into the architecture behind modern LLMs.
            Project: Implement a simple transformer model for a text-based task, such as translation or summarization.
            Skills Learned: Attention mechanisms, positional encoding, multi-head attention.
            Technologies: PyTorch, TensorFlow, Hugging Face Transformers.
            Fine-Tuning Pre-Trained Models

            Objective: Leverage pre-trained models for specific NLP tasks.
            Project: Fine-tune a pre-trained BERT or GPT model for a custom text classification task.
            Skills Learned: Transfer learning, fine-tuning, handling large-scale pre-trained models.
            Technologies: Hugging Face Transformers, PyTorch, TensorFlow.
            Building a Basic Chatbot

            Objective: Create an interactive chatbot with basic conversational capabilities.
            Project: Develop a simple rule-based chatbot to handle predefined interactions.
            Skills Learned: Basic NLP techniques, intent recognition, response generation.
            Technologies: NLTK, Rasa, Python.
            Developing a Context-Aware Chatbot

            Objective: Enhance the chatbot with context management for more natural conversations.
            Project: Implement a context-aware chatbot using an LSTM or Transformer-based model.
            Skills Learned: Context tracking, managing stateful interactions.
            Technologies: Rasa, PyTorch, TensorFlow.
            Integrating a Pre-Trained LLM into the Chatbot

            Objective: Leverage a pre-trained LLM for sophisticated responses.
            Project: Integrate GPT-3 (via OpenAI API) or another LLM into your chatbot for more advanced conversations.
            Skills Learned: API integration, managing API limitations, ensuring coherent responses.
            Technologies: OpenAI API, Hugging Face Transformers, Flask/Django for web integration.
            Polishing the User Interface

            Objective: Make the chatbot user-friendly and visually appealing.
            Project: Develop a web or mobile interface for your chatbot.
            Skills Learned: Front-end development, integrating back-end AI models with the UI.
            Technologies: React.js, Vue.js, HTML/CSS, Flask/Django, RESTful APIs.
            Deployment and Scaling

            Objective: Deploy the chatbot and ensure it can handle multiple users.
            Project: Deploy your chatbot on a cloud platform, ensure it is scalable and reliable.
            Skills Learned: Cloud deployment, containerization (Docker), orchestration (Kubernetes).
            Technologies: AWS/GCP/Azure, Docker, Kubernetes.
    
    --

    TODO, LLM chat bot, step 1, Basic Neural Network Implementation
        Basic Neural Network Implementation

        Objective: Understand the fundamentals of neural networks.
        Project: Implement a simple feedforward neural network from scratch.
        Skills Learned: Backpropagation, gradient descent, activation functions.
        Technologies: Pure Python, NumPy.

        --

        This seems easier to manage on github
            see:
            https://github.com/pjmattingly/1-Basic-Neural-Network-Implementation/tree/main

        --

        Then drilling down to the first step.
            see:
            https://github.com/pjmattingly/1-Basic-Neural-Network-Implementation/tree/main/1-Initialize-the-Network

        And setting up a plan for the code.
            see:
            https://github.com/pjmattingly/1-Basic-Neural-Network-Implementation/blob/main/1-Initialize-the-Network/Code_Structure.md

        Then the local repo
            see:
            D:\Github\1-Basic-Neural-Network-Implementation

            and:
            D:\Github\1-Basic-Neural-Network-Implementation\1-Initialize-the-Network

        --

        [06.29.24]

        --

        what sort of resources can I reference for starting development?
            books, courses, etc.

            --

            recommendations
                To get started on the "Basic Neural Network Implementation" step, it's beneficial to have a solid understanding of the underlying theory. Here are some recommended resources that cover the essential theoretical background:

                ### Books

                1. **"Deep Learning" by Ian Goodfellow, Yoshua Bengio, and Aaron Courville**
                - Comprehensive introduction to the fundamentals of deep learning, including neural networks, backpropagation, and various architectures.

                2. **"Neural Networks and Deep Learning" by Michael Nielsen**
                - Free online book that provides a clear and intuitive introduction to the principles of neural networks and deep learning.

                3. **"Pattern Recognition and Machine Learning" by Christopher Bishop**
                - Covers a broad range of machine learning topics, including neural networks, from a probabilistic perspective.

                ### Online Courses

                1. **Coursera: Deep Learning Specialization by Andrew Ng (deeplearning.ai)**
                - Series of courses covering neural networks, deep learning, and related topics. Highly recommended for a structured learning path.

                2. **edX: Machine Learning with Python: from Linear Models to Deep Learning by MIT**
                - Comprehensive course that covers the fundamentals of machine learning and deep learning.

                3. **Udacity: Deep Learning Nanodegree**
                - In-depth program focused on neural networks and deep learning, including practical projects.

                ### Tutorials and Articles

                1. **3Blue1Brown: Neural Networks (YouTube Series)**
                - Visual and intuitive explanations of neural networks and backpropagation.

                2. **Khan Academy: Neural Networks**
                - Introduction to the basics of neural networks with interactive visualizations.

                3. **Towards Data Science (Medium): Various Articles on Neural Networks**
                - Platform with numerous articles explaining different aspects of neural networks and deep learning.

                ### Research Papers

                1. **"Gradient-Based Learning Applied to Document Recognition" by Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner**
                - Seminal paper on convolutional neural networks, providing insights into early neural network research.

                2. **"Efficient BackProp" by Yann LeCun, Léon Bottou, Genevieve B. Orr, and Klaus-Robert Müller**
                - Detailed explanation of backpropagation and its optimization.

                ### Documentation and Libraries

                1. **NumPy Documentation**
                - Essential for understanding how to perform mathematical operations efficiently in Python.

                2. **TensorFlow and PyTorch Documentation**
                - Useful for understanding how to implement neural networks using popular deep learning frameworks.

                ### Summary

                By utilizing these resources, you'll gain a thorough understanding of the theory behind neural networks, which will be invaluable as you implement your own basic neural network from scratch. This foundational knowledge will also help you tackle more advanced topics and projects in the future

            --

            Then reviewing book recommendations
                Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep learning. MIT press.

                --

                well cited
                positive reviews
                I actually purchased it one point
                    see:
                    https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=Deep+Learning+Ian+Goodfellow&btnG=

                    and:
                    https://www.reddit.com/r/MachineLearning/comments/4dt4gt/deep_learning_book_by_ian_goodfellow_yoshua/

                    and:
                    https://www.amazon.ca/Deep-Learning-Ian-Goodfellow/dp/0262035618

                    and:
                    https://books.google.ca/books/about/Deep_Learning.html?id=-s2MEAAAQBAJ&source=kp_book_description&redir_esc=y

                And so seems like a good resource moving forward
        DONE
            Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep learning. MIT press.

            This seems like a good fit.
                see:
                C:\Users\peter-work\Calibre Library\Goodfellow, Ian; Bengio, Yoshua; Cou\Deep Learning (19)\Deep Learning - Goodfellow, Ian; Bengio, Yoshua.pdf
        
        --

        is there a good method for integrating development and research notes in association with the code?
            It would be nice to keep such notes in the repo itself instead of separately
        DONE
            a markdown file in the repo should be fine

        --

        also, this might be a good place to take a closer look at modern software development practices, so I can implement them here
            Agile seems like overkill, but it might be nice to have a simpler and more modern method for development.
        DONE
            my standard approach of iterative development coupled with Test Driven Development seems fine

        --

        maybe try out asking for code review from ChatGPT in between implementing major features?
            asking about its capabilities
                ChatGPT has several capabilities that can be highly useful for code review. While it cannot execute or test the code directly, it can analyze and provide feedback based on the code provided to it. Here’s how ChatGPT can assist with code review:

                ### Capabilities

                1. **Syntax and Style Checking:**
                - **Syntax Issues:** Identify and point out syntax errors or inconsistencies in the code.
                - **Coding Standards:** Provide feedback on adherence to coding standards and best practices (e.g., PEP 8 for Python).

                2. **Logical and Structural Analysis:**
                - **Logic Flaws:** Highlight potential logical errors or areas where the logic might not produce the intended results.
                - **Structure and Organization:** Suggest improvements in code organization and structure for better readability and maintainability.

                3. **Efficiency and Optimization:**
                - **Performance:** Identify potential performance bottlenecks and suggest optimizations.
                - **Algorithm Efficiency:** Provide feedback on the efficiency of algorithms used and suggest more optimal approaches if applicable.

                4. **Documentation and Comments:**
                - **Docstrings and Comments:** Review and suggest improvements for in-code documentation and comments.
                - **Clarity:** Ensure that the code is well-documented and that comments enhance understanding without being redundant.

                5. **Security Considerations:**
                - **Vulnerabilities:** Point out potential security vulnerabilities in the code.
                - **Best Practices:** Suggest security best practices to follow (e.g., input validation, avoiding hard-coded credentials).

                6. **Code Readability:**
                - **Naming Conventions:** Suggest improvements to variable, function, and class names for better clarity.
                - **Code Clarity:** Provide feedback on making the code more readable and maintainable by suggesting better structuring or refactoring.

                ### Example Code Review Process with ChatGPT

                1. **Submit Code for Review:**
                - Provide a snippet or full block of code that you want reviewed.

                2. **Identify Issues and Suggestions:**
                - ChatGPT will analyze the code and identify potential issues, offering suggestions for improvements. This might include syntax errors, logic flaws, or inefficiencies.

                3. **Request Specific Feedback:**
                - You can ask for feedback on specific aspects of the code, such as algorithm efficiency, security practices, or adherence to coding standards.

                4. **Iterative Feedback:**
                - Make changes based on the feedback and resubmit the code for further review. This iterative process can help refine and improve the code quality.

                ### Limitations

                While ChatGPT is powerful, it does have limitations:
                - **Context Awareness:** ChatGPT may not have full context if only a part of the codebase is provided, potentially missing dependencies or interactions with other parts of the code.
                - **Dynamic Analysis:** It cannot run the code, so it relies on static analysis and cannot identify runtime issues.
                - **Specific Language Nuances:** While ChatGPT supports many programming languages, there may be nuances in certain languages or frameworks that it might not fully capture.

                ### Practical Example

                Suppose you provide the following Python code snippet for review:

                ```python
                def calculate_sum(numbers):
                    result = 0
                    for num in numbers:
                        result += num
                    return result

                numbers = [1, 2, 3, 4, 5]
                print(calculate_sum(numbers))
                ```

                **ChatGPT’s Review:**

                1. **Correctness:**
                - The function `calculate_sum` correctly sums up the elements of the list `numbers`.

                2. **Optimization:**
                - The function can be optimized using Python’s built-in `sum()` function for better readability and performance.
                - Suggested Improvement: `return sum(numbers)`

                3. **Documentation:**
                - Adding a docstring to the function improves readability and maintainability.
                - Suggested Improvement:
                    ```python
                    def calculate_sum(numbers):
                        """
                        Calculate the sum of a list of numbers.
                        
                        Parameters:
                        numbers (list): A list of numeric values.
                        
                        Returns:
                        int: The sum of the numbers in the list.
                        """
                        return sum(numbers)
                    ```

                4. **Variable Naming:**
                - The variable names are clear, but consider renaming `result` to something more descriptive if the function were more complex.

                By following these suggestions, the revised code might look like:

                ```python
                def calculate_sum(numbers):
                    """
                    Calculate the sum of a list of numbers.
                    
                    Parameters:
                    numbers (list): A list of numeric values.
                    
                    Returns:
                    int: The sum of the numbers in the list.
                    """
                    return sum(numbers)

                numbers = [1, 2, 3, 4, 5]
                print(calculate_sum(numbers))
                ```

                ### Conclusion

                ChatGPT can provide valuable insights and suggestions for improving code through static analysis. By leveraging its capabilities, you can enhance code quality, readability, and maintainability, ultimately leading to better software development practices. However, it’s important to complement this with dynamic testing and real-world usage to ensure comprehensive code quality.

            --

            and so it seems best to ask for code review for specific, self-contained blocks of code

            it's not clear how it would handle me submitting my entire code-base and asking for feedback on specific parts, and so seems worth testing.

        --

        Then given the test-centric approach, what test-framework would be useful to work with?
            discussion
                When developing a neural network project in Python, it's crucial to use a robust testing framework to ensure the correctness and reliability of your code. Here are some popular and reasonable test frameworks for Python, along with their features and use cases:

                ### 1. **unittest**
                - **Description:** The built-in Python testing framework, also known as `PyUnit`.
                - **Features:**
                - Comes with Python’s standard library, no additional installation required.
                - Supports test discovery, test fixtures, and test suites.
                - Integrates well with many continuous integration tools.
                - **Use Cases:** Suitable for unit testing small to medium-sized projects where you need to write simple test cases.
                - **Example:**
                ```python
                import unittest

                class TestNeuron(unittest.TestCase):
                    def test_forward_pass(self):
                        neuron = Neuron(3)  # assuming a Neuron class is defined
                        output = neuron.forward([1.0, 2.0, 3.0])
                        self.assertIsNotNone(output)

                if __name__ == '__main__':
                    unittest.main()
                ```

                ### 2. **pytest**
                - **Description:** A more powerful and flexible testing framework that supports fixtures, parameterized testing, and plugins.
                - **Features:**
                - Simple syntax, making it easy to write concise test cases.
                - Extensive plugin architecture (e.g., pytest-cov for coverage).
                - Detailed assertion introspection, showing useful information when tests fail.
                - Support for test discovery.
                - **Use Cases:** Ideal for both small and large projects, especially when you need advanced features like fixtures and parameterized tests.
                - **Example:**
                ```python
                def test_forward_pass():
                    neuron = Neuron(3)
                    output = neuron.forward([1.0, 2.0, 3.0])
                    assert output is not None
                ```

                ### 3. **doctest**
                - **Description:** Tests embedded in docstrings that verify that interactive examples in docstrings produce the expected results.
                - **Features:**
                - Allows you to test code examples in documentation.
                - Simple to set up and use.
                - **Use Cases:** Useful for ensuring that code examples in your documentation remain correct.
                - **Example:**
                ```python
                def add(a, b):
                    """
                    Adds two numbers.

                    >>> add(2, 3)
                    5
                    >>> add('a', 'b')
                    'ab'
                    """
                    return a + b

                if __name__ == "__main__":
                    import doctest
                    doctest.testmod()
                ```

                ### 4. **nose2**
                - **Description:** Successor to the `nose` framework, designed to extend unittest to make testing easier.
                - **Features:**
                - Extends unittest with additional features like test discovery and plugins.
                - Compatible with unittest tests.
                - **Use Cases:** Useful if you need unittest compatibility with additional features.
                - **Example:**
                ```python
                import unittest

                class TestNeuron(unittest.TestCase):
                    def test_forward_pass(self):
                        neuron = Neuron(3)
                        output = neuron.forward([1.0, 2.0, 3.0])
                        self.assertIsNotNone(output)

                if __name__ == '__main__':
                    unittest.main()
                ```

                ### 5. **Hypothesis**
                - **Description:** A property-based testing library that generates test cases for you.
                - **Features:**
                - Automatically generates a wide range of input data to test edge cases.
                - Integrates with pytest for more comprehensive testing.
                - **Use Cases:** Ideal for testing functions with complex input spaces where edge cases might be overlooked.
                - **Example:**
                ```python
                from hypothesis import given
                from hypothesis.strategies import lists, floats

                @given(lists(floats(min_value=0, max_value=1), min_size=3, max_size=3))
                def test_forward_pass(inputs):
                    neuron = Neuron(3)
                    output = neuron.forward(inputs)
                    assert output is not None
                ```

                ### Conclusion

                For a neural network project in Python, using a combination of these frameworks can be beneficial:

                - **unittest** or **pytest** for general unit testing and integration tests.
                - **doctest** for verifying code examples in documentation.
                - **Hypothesis** for property-based testing to ensure robustness against a wide range of inputs.

                Among these, **pytest** is highly recommended due to its simplicity, flexibility, and powerful features, making it suitable for projects of all sizes.
        DONE
            pytest looks good
                see:
                https://docs.pytest.org/en/8.2.x/

        --

        Then, since we need numpy and pytest for the project, what sort of package management would be useful?
        DONE
            poetry looks good
                see:
                https://python-poetry.org/
        
        --

        then doing initial setup before starting coding
            TODO file tracking
                see:
                https://github.com/pjmattingly/1-Basic-Neural-Network-Implementation/blob/main/TODO%2C%20LLM%20chatbot.txt

            Install Poetry
                they recommend using pipx
                    see:
                    https://python-poetry.org/docs/

                then pipx recommends using "scoop" to install
                    see:
                    https://pipx.pypa.io/stable/installation/

                then scoop is another package manager like chocolatey
                    see:
                    https://scoop.sh/

                    and:
                    https://en.wikipedia.org/wiki/Chocolatey

                    and:
                    https://en.wikipedia.org/wiki/Scoop_Package_Manager

                then, since there's a new package manager, what sort of shell should I be using for development?
                    see:
                    https://www.reddit.com/r/learnprogramming/comments/zbssou/good_terminals_for_windows/

                    and:
                    https://www.reddit.com/r/learnprogramming/comments/1ag3g86/what_do_developers_using_windows_use_as_shell/

                    and:
                    https://en.wikipedia.org/wiki/Windows_Terminal
                DONE
                    then consensus seems to be Windows Terminal, instead of the venerable cmder

                and Windows Terminal installed scoop without a hitch, and seems to have standard Linux commands, and tab completion
            DONE

            then, getting Windows Terminal to recognize git
            DONE
                see:
                https://dev.to/yuta28/customize-windows-terminal-and-git-operations-4el2

            --

            [06.30.24]

            --

            setting up numpy and pytest in "1-Initialize-the-Network"
            DONE
                see:
                https://python-poetry.org/docs/basic-usage/

            --

            TODO, before development, review: "Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep learning. MIT press."
                first, checking on the structure of artificial neurons; Why are they structured the way they are?
                    "Some of the earliest learning algorithms we recognize today were intended
                    to be computational models of biological learning, that is, models of how
                    learning happens or could happen in the brain."

                    "As a result, one of the names
                    that deep learning has gone by is artificial neural networks (ANNs)."

                    "While the kinds of neural networks used for machine
                    learning have sometimes been used to understand brain function (Hinton and
                    Shallice, 1991), they are generally not designed to be realistic models of
                    biological function."

                    "multiple levels of composition" ?
                        "e modern term “deep learning” goes beyond the neuroscientific
                        perspective on the current breed of machine learning models. It appeals to a
                        more general principle of learning multiple levels of composition, which can be
                        applied in machine learning frameworks that are not necessarily neurally
                        inspired."

                    --

                    "e training algorithm used to adapt the weights of the
                    ADALINE was a special case of an algorithm called stochastic gradient
                    descent."

                    "Slightly modified versions of the stochastic gradient descent algorithm
                    remain the dominant training algorithms for deep learning models today."

                    --

                    "Most neural networks today are based
                    on a model neuron called the rectified linear unit."

                    "e simplified modern version
                    was developed incorporating ideas from many viewpoints, with Nair and
                    Hinton (2010) and Glorot et al. (2011a) citing neuroscience as an influence,
                    and Jarrett et al. (2009) citing more engineering-oriented influences"

                    and so modern logical neurons are "evolved" from earlier more complex representations, as they seem to be the simplest logical constructs that perform well at the kinds of tasks associated with machine learning
                        "We know that actual neurons compute very different functions
                        than modern rectified linear units, but greater neural realism has not yet led to
                        an improvement in machine learning performance."

                    --

                    "Another major accomplishment of the connectionist movement was the
                    successful use of back-propagation to train deep neural networks with internal
                    representations and the popularization of the back-propagation algorithm
                    (Rumelhart et al., 1986a; LeCun, 1987)."

                    --

                    " Hochreiter and Schmidhuber (1997)
                    introduced the long short-term memory (LSTM) network to resolve some of
                    these difficulties."

                    "Today, the LSTM is widely used for many sequence modeling
                    tasks, including many natural language processing tasks at Google."

                    --

                    "At this time, deep neural networks outperformed competing AI
                    systems based on other machine learning technologies as well as hand-designed
                    functionality."

                    "e third wave began with a focus
                    on new unsupervised learning techniques and the ability of deep models to
                    generalize well from small datasets, but today there is more interest in much
                    older supervised learning algorithms and the ability of deep models to leverage
                    large labeled datasets."

                    ----

                    "Another crowning achievement of deep learning is its extension to the
                    domain of reinforcement learning."

                    "In the context of reinforcement learning,
                    an autonomous agent must learn to perform a task by trial and error, without
                    any guidance from the human operator."

                    ----

                    "By adding more layers and more units within a layer, a deep network
                    can represent functions of increasing complexity"

                    "Most tasks that consist of
                    mapping an input vector to an output vector, and that are easy for a person to
                    do rapidly, can be accomplished via deep learning, given sufficiently large
                    models and sufficiently large datasets of labeled training examples"

                    ----

                    "Deep feedforward networks, also called feedforward neural networks, or
                    multilayer perceptrons (MLPs), are the quintessential deep learning models."

                    "e goal of a feedforward network is to approximate some function f*. For
                    example, for a classifier, y = f*(x) maps an input x to a category y. A feedforward
                    network defines a mapping y = f(x;
                    θ) and learns the value of the parameters
                    θ
                    that result in the best function approximation."

                    --

                    "ese models are called feedforward because information flows through the
                    function being evaluated from x, through the intermediate computations used
                    to define f, and finally to the output y."

                    "ere are no feedback connections in
                    which outputs of the model are fed back into itself."

                    --

                    "Feedforward neural networks are called networks because they are typically
                    represented by composing together many different functions. e model is
                    associated with a directed acyclic graph describing how the functions are
                    composed together. For example, we might have three functions f(1), f(2), and
                    f(3) connected in a chain, to form f(x) = f(3)(f(2)(f(1)(x))). ese chain
                    structures are the most commonly used structures of neural networks"

                    " In this
                    case, f(1) is called the first layer of the network, f(2) is called the second layer,
                    and so on. e overall length of the chain gives the depth of the model."

                    "e final layer of a
                    feedforward network is called the output layer."

                    --

                    "During neural network
                    training, we drive f(x) to match f*(x). e training data provides us with noisy,
                    approximate examples of f*(x) evaluated at different training points."

                    "e learning algorithm must decide how to use those layers to
                    produce the desired output, but the training data do not say what each
                    individual layer should do."

                    "Instead, the learning algorithm must decide how to
                    use these layers to best implement an approximation of f*."

                    "Because the training
                    data does not show the desired output for each of these layers, they are called
                    hidden layers."

                    --

                    "Finally, these networks are called neural because they are loosely inspired by
                    neuroscience. Each hidden layer of the network is typically vector valued. e
                    dimensionality of these hidden layers determines the width of the model. Each
                    element of the vector may be interpreted as playing a role analogous to a
                    neuron"

                    "Rather than thinking of the layer as representing a single vector-to-
                    vector function, we can also think of the layer as consisting of many units that
                    act in parallel, each representing a vector-to-scalar function."

                    "Each unit
                    resembles a neuron in the sense that it receives input from many other units and
                    computes its own activation value."

                    "e choice of the functions
                    f(i)(x) used to compute these representations is also loosely guided by
                    neuroscientific observations about the functions that biological neurons
                    compute."

                    --

                    "It is best to think of feedforward networks as
                    function approximation machines that are designed to achieve statistical
                    generalization, occasionally drawing some insights from what we know about
                    the brain, rather than as models of brain function."

                    --

                    "To extend linear models to represent nonlinear functions of x, we can apply
                    the linear model not to x itself but to a transformed input
                    ϕ(x), where
                    ϕ is a
                    nonlinear transformation."

                    e strategy of deep learning is to learn
                    ϕ

                    "In this approach, we have
                    a model y = f(x;
                    θ, w) =
                    ϕ(x;
                    θ)⊤w. We now have parameters
                    θ that
                    we use to learn
                    ϕ from a broad class of functions, and parameters w
                    that map from
                    ϕ(x) to the desired output."

                    "is is an example of a
                    deep feedforward network, with
                    ϕ defining a hidden layer."

                    " In this
                    approach, we parametrize the representation as
                    ϕ(x;
                    θ) and use the
                    optimization algorithm to find the
                    θ that corresponds to a good
                    representation"

                    " If we wish, this approach can capture the benefit of
                    the first approach by being highly generic—we do so by using a very
                    broad family
                    ϕ(x;
                    θ)."

                    "Human practitioners can encode their
                    knowledge to help generalization by designing families
                    ϕ(x;
                    θ) that
                    they expect will perform well."

                    --

                    "Feedforward networks are the application of this
                    principle to learning deterministic mappings from x to y that lack feedback
                    connections"

                    --

                    "First, training a feedforward network requires making many of the
                    same design decisions as are necessary for a linear model: choosing the
                    optimizer, the cost function, and the form of the output units. "

                    "We review these
                    basics of gradient-based learning, then proceed to confront some of the design
                    decisions that are unique to feedforward networks."

                    "Feedforward networks have
                    introduced the concept of a hidden layer, and this requires us to choose the
                    activation functions that will be used to compute the hidden layer values."

                    "We
                    must also design the architecture of the network, including how many layers the
                    network should contain, how these layers should be connected to each other,
                    and how many units should be in each layer."

                    "Learning in deep neural networks
                    requires computing the gradients of complicated functions"

                    "We present the
                    back-propagation algorithm and its modern generalizations, which can be used
                    to efficiently compute these gradients"

                    ----

                    "Now we must choose the form of our model, f(x;
                    θ). Suppose that we
                    choose a linear model, with
                    θ consisting of w and b."

                    "We can minimize J(
                    θ) in closed form with respect to w and b using the normal
                    equations."

                    Here (pg 224) they talk about a function f, s.t. f(x; w, b) = (x^T)w + b, which are the weights and biases associated with a neuron. And so, each neuron is structured this way to be the simplest linear model with parameters: weights (w) and biases(b). The "knobs" that can be changed are `w` and `b` which serve different purposes in the linear model, and where the model can't be made simpler and still be a "fully configurable" linear model.

                    --

                    "We use that strategy here, by defining h = g(W⊤x + c), where W provides the
                    weights of a linear transformation and c the biases. Previously, to describe a
                    linear regression model, we used a vector of weights and a scalar bias parameter
                    to describe an affine transformation from an input vector to an output scalar.
                    Now, we describe an affine transformation from a vector x to a vector h, so an
                    entire vector of bias parameters is needed."

                    "e activation function g is typically
                    chosen to be a function that is applied element-wise, with hi = g(x⊤W:,i + ci)"

                    "In modern neural networks, the default recommendation is to use the rectified
                    linear unit, or ReLU (Jarrett et al., 2009; Nair and Hinton, 2010; Glorot et al.,
                    2011a), defined by the activation function g(z) = max{0, z} ..."

                    and then the activation function is the bit that takes the linear model of the neuron as input and outputs non-linearity. Or the activation function introduces non-linearity to the linear model of the neuron as described by `f(x; w, b) = (x^T)w + b`.

                    --

                    "Much as a Turing
                    machine’s memory needs only to be able to store 0 or 1 states, we can build a
                    universal function approximator from rectified linear functions."

                    --

                    and stopped at pg 228, as then it starts talking about how to make the neural network function, and we want to just describe the smallest bit for now.
                DONE
                    modern logical neurons are "evolved" from earlier more complex representations, more closely related to biological neurons
                    and modern (artificial) neurons are seemingly the simplest logical constructs that can be used to construct a universal function approximater
                    the simplest neural network that we're going to look at is the "Deep feedforward network"
                    where:
                        ```
                        The goal of a feedforward network is to approximate some function f*.
                        For example, for a classifier, y = f*(x) maps an input x to a category y.
                        A feedforward network defines a mapping y = f(x; θ) and learns the value of the parameters θ
                        that result in the best function approximation.
                        ```
                    Where such a network is composed of:
                        input layer
                        hidden layers
                        output layer
                    Where each layer is a set of neurons that are densly connected with the next.

                    "It is best to think of feedforward networks as
                    function approximation machines that are designed to achieve statistical
                    generalization, occasionally drawing some insights from what we know about
                    the brain, rather than as models of brain function."

                    "To extend linear models to represent nonlinear functions of x, we can apply the linear model not to x itself but to a transformed input ϕ(x), where ϕ is a nonlinear transformation."

                    " ... training a feedforward network requires making many of the
                    same design decisions as are necessary for a linear model: choosing the
                    optimizer, the cost function, and the form of the output units. "

                    "Feedforward networks have introduced the concept of a hidden layer, and this requires us to choose the activation functions that will be used to compute the hidden layer values."

                    And so each neuron is structured as a linear model (f(x; w, b) = (x^T)w + b; where `w` is a vector of weights and `b` is a vector of biases), which acts as input to a non-linear activation function `g`. Which then fulfills the "extend linear models to represent nonlinear functions", via "apply the linear model not to x itself but to a transformed input ϕ(x), where ϕ is a nonlinear transformation". Where the linear model is the neuron and the activation function is the "nonlinear transformation".

                    They recommend using "rectified linear unit, or ReLU (Jarrett et al., 2009; Nair and Hinton, 2010; Glorot et al., 2011a), defined by the activation function g(z) = max{0, z}" as the activation function.

                    And "Much as a Turing machine’s memory needs only to be able to store 0 or 1 states, we can build a universal function approximator from rectified linear functions."
            DONE

            then with the theory out of the way, we can move on to the mechanics.

            --

            Artificial neuron
            https://en.wikipedia.org/wiki/Artificial_neuron
                    "An artificial neuron is a mathematical function conceived as a model of biological neurons in a neural network. Artificial neurons are the elementary units of artificial neural networks."

                    "The artificial neuron is a function that receives one or more inputs, applies weights to these inputs, and sums them to produce an output. "

                    "Usually, each input is separately weighted, and the sum is often added to a term known as a bias (loosely corresponding to the threshold potential), before being passed through a non-linear function known as an activation function or transfer function ... "

                    "The transfer functions usually have a sigmoid shape, but they may also take the form of other non-linear functions, piecewise linear functions, or step functions. They are also often monotonically increasing, continuous, differentiable and bounded."

                    For a given artificial neuron k, let there be m + 1 inputs with signals x0 through xm and weights wk0 through wkm. Usually, the x0 input is assigned the value +1, which makes it a bias input with wk0 = bk. This leaves only m actual inputs to the neuron: from x1 to xm.

                    The output of the kth neuron is:

                        y k = φ ( ∑ j = 0 m w k j x j ) {\displaystyle y_{k}=\varphi \left(\sum _{j=0}^{m}w_{kj}x_{j}\right)}

                    Where φ {\displaystyle \varphi } (phi) is the transfer function (commonly a threshold function).

                    --

                    "Any finite state machine can be simulated by a MCP neural network.[4] Furnished with an infinite tape, MCP neural networks can simulate any Turing machine."

                    --

                    pseudo code
                        see:
                        https://en.wikipedia.org/wiki/Artificial_neuron#Pseudocode_algorithm
        DONE
            using Poetry for Python package management
            installed scoop to install Windows packages
            using Windows Terminal for a terminal app
            using numpy and pytest for development during this step
            theory needed for the "Neuron" class
                And so each neuron is structured as a linear model (f(x; w, b) = (x^T)w + b; where `w` is a vector of weights and `b` is a vector of biases), which acts as input to a non-linear activation function `g`. Which then fulfills the "extend linear models to represent nonlinear functions", via "apply the linear model not to x itself but to a transformed input ϕ(x), where ϕ is a nonlinear transformation". Where the linear model is the neuron and the activation function is the "nonlinear transformation".

        --

        TODO, 1, Initialize the Network
            see:
            https://github.com/pjmattingly/1-Basic-Neural-Network-Implementation/blob/main/README.md

            and:
            https://github.com/pjmattingly/1-Basic-Neural-Network-Implementation/blob/main/1-Initialize-the-Network/Code_Structure.md

            and:
            https://github.com/pjmattingly/1-Basic-Neural-Network-Implementation/blob/main/1-Initialize-the-Network/1_initialize_the_network/Neuron.py

            --

            then type checking, how does modern Python do it?
            DONE
                annotations
                    see:
                    https://realpython.com/python-type-checking/

                and:
                    https://docs.python.org/3/library/typing.html
            
            hmm but that doesn't throw a type error when doing something like:
                `Neuron("something")`
            
            rather they're type "hints" and for linters, not enforcement for specific types passed in
                see:
                https://stackoverflow.com/questions/67412233/python-type-hinting-not-generating-error-for-wrong-type-when-running-the-code

            and so when doing exceptions for bad input, we have to check it manually
                see:
                https://realpython.com/python-type-checking/#dynamic-typing

            --

            [07.03.24]

            --

            setting up first tests for Neuron
                see:
                https://docs.pytest.org/en/8.2.x/getting-started.html#getstarted

            to get import working had to change the name of the directory containing Neuron
                Poetry automatically made: "1_initialize_the_network" instead of something that could be imported





            

        next,
            

        then,
            docstrings are best practices, is there a way to automatically create them?
                see:
                https://www.datacamp.com/tutorial/docstrings-python

                maybe:
                https://marketplace.visualstudio.com/items?itemName=njpwerner.autodocstring

        then,
            take a closer look at modern coding practices, should I be making better use of autocomplete or AI?
            what's the standard tool chain for developing in Python?
                
        then,
            more reading and theory before implementing classes Layer and Network
        

----------------

TODO

----------------

NOTE
    Note, header "first, checking on the structure of artificial neurons; Why are they structured the way they are?" describes the motivation for and structure of the artificial neuron

    --

    Note, pg 228+ in "Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep learning. MIT press." provides further information on how to make the example feedforward network function
        see:
        C:\Users\peter-work\Calibre Library\Goodfellow, Ian; Bengio, Yoshua; Cou\Deep Learning (19)\Deep Learning - Goodfellow, Ian; Bengio, Yoshua.pdf

    --

    Note, chatgpt chat
        see:
        https://chatgpt.com/c/5380ffd7-2b2c-4244-82f1-2ee744e67254?model=gpt-4o

    --

    Note, roadmap for developing an LLM based chat bot.
        see:
        https://github.com/pjmattingly/1-Basic-Neural-Network-Implementation/blob/main/LLM_chatbot_Roadmap.md

        --

        Basic Neural Network Implementation

        Objective: Understand the fundamentals of neural networks.
        Project: Implement a simple feedforward neural network from scratch.
        Skills Learned: Backpropagation, gradient descent, activation functions.
        Technologies: Pure Python, NumPy.

        Using a Deep Learning Framework

        Objective: Get familiar with a popular deep learning framework.
        Project: Re-implement the basic neural network using PyTorch or TensorFlow.
        Skills Learned: Using tensors, automatic differentiation, basic model training.
        Technologies: PyTorch, TensorFlow.

        Text Data Preprocessing

        Objective: Learn how to preprocess text data for NLP tasks.
        Project: Implement text preprocessing techniques such as tokenization, stemming, lemmatization, and stop words removal.
        Skills Learned: Text normalization, handling different text formats.
        Technologies: NLTK, SpaCy.

        Word Embeddings and Text Representation

        Objective: Understand how to represent text data in a way that neural networks can process.
        Project: Implement and use word embeddings like Word2Vec or GloVe for text representation.
        Skills Learned: Embedding layers, vector space models.
        Technologies: Gensim, PyTorch, TensorFlow.

        Recurrent Neural Networks (RNNs) and LSTMs

        Objective: Learn about RNNs for handling sequential data.
        Project: Build an RNN or LSTM for a simple text generation task.
        Skills Learned: Handling sequences, managing hidden states, text generation.
        Technologies: PyTorch, TensorFlow.

        Transformer Models

        Objective: Dive into the architecture behind modern LLMs.
        Project: Implement a simple transformer model for a text-based task, such as translation or summarization.
        Skills Learned: Attention mechanisms, positional encoding, multi-head attention.
        Technologies: PyTorch, TensorFlow, Hugging Face Transformers.

        Fine-Tuning Pre-Trained Models

        Objective: Leverage pre-trained models for specific NLP tasks.
        Project: Fine-tune a pre-trained BERT or GPT model for a custom text classification task.
        Skills Learned: Transfer learning, fine-tuning, handling large-scale pre-trained models.
        Technologies: Hugging Face Transformers, PyTorch, TensorFlow.

        Building a Basic Chatbot

        Objective: Create an interactive chatbot with basic conversational capabilities.
        Project: Develop a simple rule-based chatbot to handle predefined interactions.
        Skills Learned: Basic NLP techniques, intent recognition, response generation.
        Technologies: NLTK, Rasa, Python.

        Developing a Context-Aware Chatbot

        Objective: Enhance the chatbot with context management for more natural conversations.
        Project: Implement a context-aware chatbot using an LSTM or Transformer-based model.
        Skills Learned: Context tracking, managing stateful interactions.
        Technologies: Rasa, PyTorch, TensorFlow.

        Integrating a Pre-Trained LLM into the Chatbot

        Objective: Leverage a pre-trained LLM for sophisticated responses.
        Project: Integrate GPT-3 (via OpenAI API) or another LLM into your chatbot for more advanced conversations.
        Skills Learned: API integration, managing API limitations, ensuring coherent responses.
        Technologies: OpenAI API, Hugging Face Transformers, Flask/Django for web integration.

        Polishing the User Interface

        Objective: Make the chatbot user-friendly and visually appealing.
        Project: Develop a web or mobile interface for your chatbot.
        Skills Learned: Front-end development, integrating back-end AI models with the UI.
        Technologies: React.js, Vue.js, HTML/CSS, Flask/Django, RESTful APIs.

        Deployment and Scaling

        Objective: Deploy the chatbot and ensure it can handle multiple users.
        Project: Deploy your chatbot on a cloud platform, ensure it is scalable and reliable.
        Skills Learned: Cloud deployment, containerization (Docker), orchestration (Kubernetes).
        Technologies: AWS/GCP/Azure, Docker, Kubernetes.

----------------

DONE
    TODO, before development, review: "Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep learning. MIT press."
        see:
        C:\Users\peter-work\Calibre Library\Goodfellow, Ian; Bengio, Yoshua; Cou\Deep Learning (19)\Deep Learning - Goodfellow, Ian; Bengio, Yoshua.pdf
    DONE

# tail